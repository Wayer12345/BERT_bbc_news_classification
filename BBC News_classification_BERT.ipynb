{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11837c2c",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/competitions/nlp-getting-started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415403b6",
   "metadata": {},
   "source": [
    "# Проект: \"предсказание типа комментариев: о реальных катастрофах и нереальных катастрофах.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28048a57",
   "metadata": {},
   "source": [
    "# Импортирование библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "42693212",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-23T07:07:47.798Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Maxim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import transformers\n",
    "from transformers import BertModel, BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b185dd1",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-23T07:07:51.168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e20ebd",
   "metadata": {},
   "source": [
    "# Предварительный анализ и подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25a37bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('bbc-text.csv')\n",
    "train.columns = ['target', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88e6d444",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T06:57:34.058183Z",
     "start_time": "2023-11-23T06:57:34.058183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          target                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "465f2677",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T06:57:34.058183Z",
     "start_time": "2023-11-23T06:57:34.058183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Форма train: (2225, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Форма train: {}'.format(train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45ff4d22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T06:57:34.058183Z",
     "start_time": "2023-11-23T06:57:34.058183Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2225 entries, 0 to 2224\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   target  2225 non-null   object\n",
      " 1   text    2225 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 34.9+ KB\n",
      "None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('{}'.format(train.info()), end = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd84cf1",
   "metadata": {},
   "source": [
    "Тестовая выборка меток не имеет, поэтому когда будем анализировать все данные без учета меток будем анализировать df = train + test, когда будет происходить анализ с учетом меток(например, их распределение), то будем использовать только train подвыборку. Пропусков в данных нету."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6e5647f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T06:57:34.074707Z",
     "start_time": "2023-11-23T06:57:34.074707Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего меток твитов в данных: 5\n",
      "Уникальные метки твитов: ['tech' 'business' 'sport' 'entertainment' 'politics']\n",
      "\n",
      "Количество меток каждого типа твитов:\n",
      "target\n",
      "sport            511\n",
      "business         510\n",
      "politics         417\n",
      "tech             401\n",
      "entertainment    386\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Всего меток твитов в данных: {}'.format(len(train['target'].unique())))\n",
    "print('Уникальные метки твитов: {}'.format(train['target'].unique()), end = '\\n\\n')\n",
    "print('Количество меток каждого типа твитов:\\n{}'.format(train['target'].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06002785",
   "metadata": {},
   "source": [
    "Количество меток каждого типа твитов резко не отличаются, баланс классов примерно соблюдается."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f97c808",
   "metadata": {},
   "source": [
    "# Предобработка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85fcbec",
   "metadata": {},
   "source": [
    "Итак, имеется исходный датафрейм df, в котором признак text соответствует твиту определенного пользователя, то есть это тип данных object, прежде чем приступить к работе с этими твитами, нужно нормализовать текст и каждый твит представить в виде набора нормализованных слов, а также убрать слишком часто встречающиеся слова и слишком редкие слова. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d94ae5a",
   "metadata": {},
   "source": [
    "## Понижение регистра слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a65a013",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T06:57:34.074707Z",
     "start_time": "2023-11-23T06:57:34.074707Z"
    }
   },
   "outputs": [],
   "source": [
    "train['text'] = train['text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af809c04",
   "metadata": {},
   "source": [
    "## Исключение бесполезных слов и других бесполезных конструкций"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504510ef",
   "metadata": {},
   "source": [
    "### Исключение чисел и цифр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "14506dfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T06:57:34.074707Z",
     "start_time": "2023-11-23T06:57:34.074707Z"
    }
   },
   "outputs": [],
   "source": [
    "def number_remove(text):\n",
    "    line = re.sub('\\d+', '', text)\n",
    "    return line\n",
    "\n",
    "train['new_text'] = train['text'].apply(lambda x: number_remove(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecceb1e",
   "metadata": {},
   "source": [
    "### Исключение ссылок на сайты и html ссылок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "747e89a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T06:57:34.084273Z",
     "start_time": "2023-11-23T06:57:34.084273Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_remove = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_remove.sub(r'', text)\n",
    "train['new_text'] = train['new_text'].apply(lambda x:remove_urls(x))\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "train['new_text']=train['new_text'].apply(lambda x:remove_html(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389bd61f",
   "metadata": {},
   "source": [
    "### Исключение пунктуационных знаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9e3bc640",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T06:57:34.088179Z",
     "start_time": "2023-11-23T06:57:34.088179Z"
    }
   },
   "outputs": [],
   "source": [
    "def punct_remove(text):\n",
    "    punct = re.sub(r\"[^\\w\\s\\d]\",\"\", text)\n",
    "    return punct\n",
    "\n",
    "train['new_text']=train['new_text'].apply(lambda x:punct_remove(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f0e413",
   "metadata": {},
   "source": [
    "### Исключение стоп-слов(слов, которые почти не характеризуют специфику конкретного текста)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1614f180",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T06:57:34.091704Z",
     "start_time": "2023-11-23T06:57:34.091704Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Maxim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "\n",
    "train['new_text']=train['new_text'].apply(lambda x:remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92d318c",
   "metadata": {},
   "source": [
    "### Исключение хэштегов(#) и отсылок(@) в твитах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "29282732",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T06:57:34.094779Z",
     "start_time": "2023-11-23T06:57:34.094779Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_hash(x):\n",
    "    text=re.sub(r'#\\w+','',x)\n",
    "    return text\n",
    "train['new_text']=train['new_text'].apply(lambda x:remove_hash(x))\n",
    "def remove_mention(x):\n",
    "    text=re.sub(r'@\\w+','',x)\n",
    "    return text\n",
    "\n",
    "train['new_text']=train['new_text'].apply(lambda x:remove_mention(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c509e7c",
   "metadata": {},
   "source": [
    "### Удаление \"длинных пробелов\"(в случае, если при обработке таковые возникли)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ecf4429d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T06:57:34.103359Z",
     "start_time": "2023-11-23T06:57:34.103359Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_space(text):\n",
    "    space_remove = re.sub(r\"\\s+\",\" \", text).strip()\n",
    "    return space_remove\n",
    "\n",
    "train['new_text']=train['new_text'].apply(lambda x:remove_space(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ad48ca",
   "metadata": {},
   "source": [
    "# Удаление прочих ненужных конструкций"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2576731",
   "metadata": {},
   "source": [
    "## Построение словаря"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1384b0ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T06:57:34.115245Z",
     "start_time": "2023-11-23T06:57:34.115245Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_dictionary(texts):\n",
    "    \n",
    "    dictionary = {}\n",
    "    idx = 0\n",
    "    \n",
    "    for text in texts:\n",
    "        for word in text.split():\n",
    "            if word not in dictionary.keys():\n",
    "                dictionary[word] = idx\n",
    "                idx += 1\n",
    "                \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "970ad506",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T06:57:34.115245Z",
     "start_time": "2023-11-23T06:57:34.115245Z"
    }
   },
   "outputs": [],
   "source": [
    "def max_sequence_len(texts):\n",
    "    max_len = 0\n",
    "    \n",
    "    for text in texts:\n",
    "        if len(text.split()) > max_len:\n",
    "            max_len = len(text.split())\n",
    "            \n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45c92652",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T06:57:34.115245Z",
     "start_time": "2023-11-23T06:57:34.115245Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря: 30189\n",
      "Максимальное количество слов в предложении: 512\n"
     ]
    }
   ],
   "source": [
    "dictionary = build_dictionary(train['new_text'])\n",
    "max_seq_len = max_sequence_len(train['new_text'])\n",
    "max_seq_len = 512 if max_seq_len > 512 else max_seq_len\n",
    "    \n",
    "print(f'Размер словаря: {len(dictionary)}')\n",
    "print(f'Максимальное количество слов в предложении: {max_seq_len}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f646a5cb",
   "metadata": {},
   "source": [
    "## pretrained BERT model tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3329c09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "labels = {'business':0,\n",
    "          'entertainment':1,\n",
    "          'sport':2,\n",
    "          'tech':3,\n",
    "          'politics':4\n",
    "          }\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.labels = [labels[label] for label in df['category']]\n",
    "        self.texts = [tokenizer(text, \n",
    "                               padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['text']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0a710f95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T06:57:34.124081Z",
     "start_time": "2023-11-23T06:57:34.124081Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\", do_lower_case=True)\n",
    "# bert = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = 5, # The number of output labels--2 for binary classification.\n",
    "                    # Can be increased for multiclass \n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b425e6a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-23T06:57:34.129783Z",
     "start_time": "2023-11-23T06:57:34.129783Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(train, test_size=0.3, stratify=train[\"target\"], random_state = 700)\n",
    "valid_data, test_data  = train_test_split(test_data, test_size=0.5, random_state = 700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9651806f",
   "metadata": {},
   "source": [
    "# Построение предсказательной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "be3c03ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    labels_dict = {'business':0,\n",
    "                   'entertainment':1,\n",
    "                   'sport':2,\n",
    "                   'tech':3,\n",
    "                   'politics':4}\n",
    "              \n",
    "    def __init__(self, df):\n",
    "\n",
    "        self.labels = [self.labels_dict[label] for label in df['target']]\n",
    "        self.texts = [bert_tokenizer(text, padding='max_length', max_length = 512, truncation=True,\n",
    "                                return_tensors=\"pt\") for text in df['text']]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f901e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = Dataset(train_data), Dataset(valid_data), Dataset(test_data)\n",
    "train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid, batch_size=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(test, batch_size=len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5c013891",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Dataset at 0x1babfc8b950>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid.texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "026aa081",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ab235ebe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e289a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "19f0ce4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 779/779 [01:24<00:00,  9.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.788                 | Train Accuracy:  0.278                 | Val Loss:  0.694                 | Val Accuracy:  0.557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 779/779 [01:25<00:00,  9.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.404                 | Train Accuracy:  0.857                 | Val Loss:  0.231                 | Val Accuracy:  0.958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 779/779 [01:24<00:00,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.150                 | Train Accuracy:  0.983                 | Val Loss:  0.106                 | Val Accuracy:  0.976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 779/779 [01:24<00:00,  9.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.077                 | Train Accuracy:  0.988                 | Val Loss:  0.069                 | Val Accuracy:  0.982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 779/779 [01:24<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.044                 | Train Accuracy:  0.997                 | Val Loss:  0.055                 | Val Accuracy:  0.973\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_dataloader, valid_dataloader, learning_rate, epochs):\n",
    "\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                batch_loss = criterion(output, train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in valid_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label.long())\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(valid_data): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(valid_data): .3f}')\n",
    "                  \n",
    "EPOCHS = 5\n",
    "model = BertClassifier()\n",
    "LR = 1e-6\n",
    "              \n",
    "train(model, train_dataloader, valid_dataloader, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a9c02edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.961\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, test_dataloader):\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "        for test_input, test_label in test_dataloader:\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "\n",
    "            acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "            total_acc_test += acc\n",
    "    \n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "    \n",
    "evaluate(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4557d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bert_model.pickle', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "79ee5bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('STOPWORDS.pickle', 'wb') as f:\n",
    "    pickle.dump(STOPWORDS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8ea2a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bert_tokenizer.pickle', 'wb') as f:\n",
    "    pickle.dump(bert_tokenizer, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166fd2fe",
   "metadata": {},
   "source": [
    "# Inference модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2e2a8d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inference:\n",
    "    def __init__(self, texts, model, STOPWORDS, bert_tokenizer):\n",
    "        \n",
    "        \"\"\"Тексты подаются в формате pandas Series\"\"\"\n",
    "        \n",
    "        self.texts = texts\n",
    "        self.model = model\n",
    "        self.stopwords = STOPWORDS\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        \n",
    "    def preprocessing(self):\n",
    "        def number_remove(text):\n",
    "            line = re.sub('\\d+', '', text)\n",
    "            return line\n",
    "        \n",
    "        def remove_urls(text):\n",
    "            url_remove = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "            return url_remove.sub(r'', text)\n",
    "        \n",
    "        def remove_html(text):\n",
    "            html=re.compile(r'<.*?>')\n",
    "            return html.sub(r'',text)\n",
    "\n",
    "        def punct_remove(text):\n",
    "            punct = re.sub(r\"[^\\w\\s\\d]\",\"\", text)\n",
    "            return punct\n",
    "        \n",
    "        def remove_stopwords(text):\n",
    "            return \" \".join([word for word in str(text).split() if word not in self.stopwords])\n",
    "        \n",
    "        def remove_hash(x):\n",
    "            text=re.sub(r'#\\w+','',x)\n",
    "            return text\n",
    "\n",
    "        def remove_mention(x):\n",
    "            text=re.sub(r'@\\w+','',x)\n",
    "            return text\n",
    "        \n",
    "        def remove_space(text):\n",
    "            space_remove = re.sub(r\"\\s+\",\" \", text).strip()\n",
    "            return space_remove\n",
    "        \n",
    "        self.texts_prep = self.texts.str.lower()\n",
    "        self.texts_prep = self.texts_prep.apply(lambda x: number_remove(x))\n",
    "        self.texts_prep = self.texts_prep.apply(lambda x: remove_urls(x))\n",
    "        self.texts_prep = self.texts_prep.apply(lambda x: remove_html(x))\n",
    "        self.texts_prep = self.texts_prep.apply(lambda x: punct_remove(x))\n",
    "        self.texts_prep = self.texts_prep.apply(lambda x: remove_stopwords(x))\n",
    "        self.texts_prep = self.texts_prep.apply(lambda x: remove_hash(x))\n",
    "        self.texts_prep = self.texts_prep.apply(lambda x: remove_mention(x))\n",
    "        self.texts_prep = self.texts_prep.apply(lambda x: remove_space(x))\n",
    "        \n",
    "    def to_bert_format(self):\n",
    "        self.new_texts = [self.bert_tokenizer(text, padding='max_length', max_length = 512, truncation=True,\n",
    "                                              return_tensors=\"pt\") for text in self.texts_prep]\n",
    "        \n",
    "    def evaluate(self):\n",
    "        device = torch.device(\"cuda\")\n",
    "        pred_probas = []\n",
    "        pred_logits = []\n",
    "        with torch.no_grad():\n",
    "            for text in self.new_texts:\n",
    "                mask = text['attention_mask'].to(device)\n",
    "                input_id = text['input_ids'].squeeze(1).to(device)\n",
    "                output = model(input_id, mask)\n",
    "                pred_logits.append(output)\n",
    "                \n",
    "        pred_logits = [x.cpu().numpy().squeeze() for x in pred_logits]\n",
    "        pred_probas = [1 / (1 + np.exp(-x)) for x in pred_logits]\n",
    "        \n",
    "        return pred_probas, pred_logits\n",
    "    \n",
    "    def run_eval(self):\n",
    "        self.preprocessing()\n",
    "        self.to_bert_format()\n",
    "        self.predictions_proba, self.predictions_logit = self.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "74475897",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('bbc-text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "90abadc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_data = Inference(data['text'], model, STOPWORDS, bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "dc0cdd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_data.run_eval()"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKcAAABACAYAAAB/V2fWAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAd4SURBVHhe7Zw5UxRbGIYPi6l/wAIBkYAMEhIKFPVPoICyGJOzCOg1tIoicwREQiPSYXEhvAExm2xlcquoIkZgbj/HPnP7jj1Lz3QPZ5rvqeqa6ZVe3n7P933nDFUpByUIFlLtfgqCdYg4BWsRcQrWIuIUrEXEKViLiFOwFhGnYC0iTsFaRJyCtYg4BWsRcVY49D5fXV2pi4sLNT8/r6qqqvR8HJC+9QqGR4cg37x5o0UJh4eH6vLyUlVXV77viHNWOI2NjWphYUElk0l3SXwQ54wJNOX37t0T5xSEciDiFKxFxBlD4pKtizhjSBziTRBxCtYi4hSsRcQpWIuIU7CWsomTDDKRSLhzwfnx44f68OFDbDLRsKAPhaI7nxTgYX9/P72skimLOLlJL168UCcnJ+6S4Ny9e1f3IXPjhf+YnJxUzc3Nqra21l2iVEtLi55/9epVRQs08u5LDj81NaWWlpbU3t7e/25iUBDm48eP1dramu6qE363SNxjM/DDYJZVclkpcnEiKN7s3d1d/VkqOMX379/VxsZGbOp5gj+RPl3e6k+fPukmuampyV1aGgMDA+ro6EgdHBy4S4S4Eqk4EdDMzIzq7+8PzeUQeVdXlxocHKzoeErIT9GKMbEOk/e7VzAmeyQwzwb7klmaY5hl3nkvxFHd3d3q+PjYd70QHwLHnIiGwa2bm5u6efXjy5cvWkTEh8vLyzru9HNOjkWyxDaAwz579iw9spuJ8lFNTY1ebyDefPTokU6wJDGKL4HEiciGhobUt2/ftHshTuOOzCOm+vp69fHjR+1qPT09ej01ykxxIkzKSw0NDdpZ2eb+/ft6HkESEoyMjOjMHCF6CZJkmRcgKFzH8PCwOydcBwWLk6YXkSA2U8rhwRP7kfTs7OxocbEMIfKDK7ZHsMZJvayvr+t49OvXr3odQqM+h3suLi7qfVnn5478DdyU388g8Mxje0Hk/H0/uPRs+xLXhlFdEIqnIHGyCe6DmDLFgkBwmNXVVV2DNCAgtuMh46R+4nzw4EG6ySZUwJU5HoLHScnyM5t0yHfscvLy5Uv3m5BJa2urGh0ddeeCU5A4cU2K54gFJ/KKgbjy9evX6vz8XN26dctdGkxAnMLz58+1AxcSRxrnxGWvW5yUtrh24U86OjqiFyeCockmNvTGb4iEuJLYk228Lsc6HJAwwK9Z92KEzLa8CCY+5dT89jPi5MXgnHKVqTgGYUJQeBEzY12hvBRUSjJJDy7ohaaX5Kivry+rQNjXT//ErQiL5AaxsZ23Hsqyhw8f+u7rLcDnEiawLccOOnFtwjXjPPy8/Pr1K+Vk0SnHqdwlqZST8KScDD3lOIz+7oeTdaOslOOG7pLfOMLTx2OdE6umnDhTf5+YmEivd5rLVDKZ1POZOC6tt3cE7i4RKoF/Pg+mGhqbUn/97S7IQ0HOaeI7XJIyDxPNOU0fzueXtADrwet0BtaRENFPzqAQR5i63kmiRPxJKSdbs0psCpSvhPhSUMwJNLOUdhAowkIY/LeJXM0q+xBLjo+P60zcC6LmeMSrNO/ElsSGzCNUjusnek6XuNckT9leDCEGIM6ooHkmFHBc94+mvVgcUesmnVDAZgiF3r9/H9p130QiFScQHxJf8hkGxKW54tzrhHMizuYcTUxt43mWCi8czxPzYTLLwqbgZr1YODzNNs01zXa+7DoXptuSuJSY1zYobXGdQCxN1u+Is6TQg2tmLAFduTbgiFDXlhl4wzX29vbqXzhEMsYWcUYNbxdu4s32g4IDUR0gQ7e1qeS8zBSWc3K93DtbSCQS6SqJk7jqZ7K9va2vlZArTCJ3TgNvHN2fJDPF9Ojglpyq6a+3GZMIhuGcdOtyHO5dqXD/zOP2OhxdxiS6uaCiwn3HyRlTy3G4RpJXM7Y27OdSNnHeJMIUJ8IhVChFnBgDlRGaXj6ptmAS5tcJnG8+MptrrovuaiomXGsUhBggCFFQqhshHlobxj9Q/kPsiJJ5I0qEl28y4OT07uGk4IQv+tPbrR0WIs4Yg7s9efJEO69JIkko79y5U1QzjJipV9OpggsjTI6BMDs7O92twkPEaQk4HPFc5oQgCA/81jFli8qMkNh3bGwsHS9SUXj79q16+vRpYHHioIy7/fnzp/4HGbjw9PS0ztgRfthIzBkBCCNIzMn2uI8pQ3nhGJRtMgfdGLINGeTvMpKM/XE4uoOJNfmOQDmnoOIsNyLOCAgqTsj2GIjxEC0O5Uc2gZEEMQYXMeJ2nAN/wxs/2o4065aAyLJNudZngxcDcEvjkl5hVoIniThjCoNyGElmROqFnhzGyvqNFrOJGifWCb8GcEPhoRMDksmurKy4S3//VJrBy21tbTndzo+trS11dnamxRQE/s7t27fV3Nycdsm6ujp1enqqZmdndRnp3bt3qr293d3aTsQ5Q4RY03zSc8Jkmk+a13JDfZN4E/fkR4iMl+U8yNqjyK7DRhIiywmj+9I84qCufd2IOC2HuBC3q6QsOyxEnIK1SMwpWIuIU7AWEadgLSJOwVpEnIK1iDgFS1HqX1hRYieM4XgeAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "id": "8cecb542",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec19725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6000e097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d009e8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4f0a02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3985b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
